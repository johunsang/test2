#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
í•œêµ­ì–´ Bllossom ëª¨ë¸ í…ìŠ¤íŠ¸ ìƒì„± ì˜ˆì œ
ê°„ë‹¨í•˜ê³  ì‹¤ìš©ì ì¸ í…ìŠ¤íŠ¸ ìƒì„± ë°ëª¨
"""

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
import warnings
warnings.filterwarnings('ignore')

class SimpleBlossomChat:
    def __init__(self):
        """ê°„ë‹¨í•œ í•œêµ­ì–´ í…ìŠ¤íŠ¸ ìƒì„±ê¸° ì´ˆê¸°í™”"""
        self.model_name = "Bllossom/llama-3.1-Korean-Bllossom-Vision-8B"
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        print(f"ğŸŒ¸ Bllossom ëª¨ë¸ ë¡œë”© ì¤‘... (ë””ë°”ì´ìŠ¤: {self.device})")
        
        self.load_model()
    
    def load_model(self):
        """ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ"""
        try:
            # í† í¬ë‚˜ì´ì € ë¡œë“œ
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.model_name,
                trust_remote_code=True
            )
            
            # íŒ¨ë”© í† í° ì„¤ì •
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
            
            # ëª¨ë¸ ë¡œë“œ (ë©”ëª¨ë¦¬ ì ˆì•½ ì„¤ì •)
            model_kwargs = {
                "trust_remote_code": True,
                "torch_dtype": torch.float16 if torch.cuda.is_available() else torch.float32,
                "low_cpu_mem_usage": True
            }
            
            # GPU ë©”ëª¨ë¦¬ ìµœì í™” (GPUê°€ ìˆëŠ” ê²½ìš°)
            if torch.cuda.is_available():
                try:
                    from transformers import BitsAndBytesConfig
                    quantization_config = BitsAndBytesConfig(
                        load_in_4bit=True,
                        bnb_4bit_compute_dtype=torch.float16,
                        bnb_4bit_use_double_quant=True,
                        bnb_4bit_quant_type="nf4"
                    )
                    model_kwargs["quantization_config"] = quantization_config
                    model_kwargs["device_map"] = "auto"
                except ImportError:
                    print("âš ï¸ bitsandbytesê°€ ì„¤ì¹˜ë˜ì§€ ì•ŠìŒ. ì¼ë°˜ ë¡œë”© ì‚¬ìš©")
                    model_kwargs["device_map"] = "auto"
            
            # ëª¨ë¸ ë¡œë“œ
            self.model = AutoModelForCausalLM.from_pretrained(
                self.model_name,
                **model_kwargs
            )
            
            print("âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ!")
            
        except Exception as e:
            print(f"âŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            print("ğŸ’¡ ì¸í„°ë„· ì—°ê²°ì„ í™•ì¸í•˜ê±°ë‚˜ ë‹¤ë¥¸ ëª¨ë¸ì„ ì‹œë„í•´ë³´ì„¸ìš”.")
            raise
    
    def generate_text(self, prompt, max_length=300, temperature=0.7):
        """í…ìŠ¤íŠ¸ ìƒì„± í•¨ìˆ˜"""
        # Llama 3.1 ì±„íŒ… í¬ë§·
        formatted_prompt = f"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n\n{prompt}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n"
        
        # í† í¬ë‚˜ì´ì§•
        inputs = self.tokenizer(
            formatted_prompt, 
            return_tensors="pt",
            truncation=True,
            max_length=512
        )
        
        # GPUë¡œ ì´ë™ (í•„ìš”í•œ ê²½ìš°)
        if torch.cuda.is_available():
            inputs = {k: v.to(self.device) for k, v in inputs.items()}
        
        # í…ìŠ¤íŠ¸ ìƒì„±
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_length=len(inputs['input_ids'][0]) + max_length,
                temperature=temperature,
                do_sample=True,
                top_p=0.9,
                top_k=50,
                repetition_penalty=1.1,
                pad_token_id=self.tokenizer.eos_token_id,
                eos_token_id=self.tokenizer.eos_token_id,
                no_repeat_ngram_size=3
            )
        
        # ë””ì½”ë”© ë° ì •ë¦¬
        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        # í”„ë¡¬í”„íŠ¸ ë¶€ë¶„ ì œê±°
        if "<|start_header_id|>assistant<|end_header_id|>" in response:
            response = response.split("<|start_header_id|>assistant<|end_header_id|>")[-1].strip()
        
        return response

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸŒ¸ í•œêµ­ì–´ Bllossom í…ìŠ¤íŠ¸ ìƒì„± ë°ëª¨")
    print("=" * 40)
    
    try:
        # ëª¨ë¸ ì´ˆê¸°í™”
        chat_bot = SimpleBlossomChat()
        
        # ìƒ˜í”Œ í”„ë¡¬í”„íŠ¸ë“¤
        sample_prompts = [
            "íŒŒì´ì¬ í”„ë¡œê·¸ë˜ë°ì˜ ì¥ì ì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
            "í•œêµ­ì˜ ì „í†µ ìŒì‹ì— ëŒ€í•´ ì†Œê°œí•´ì£¼ì„¸ìš”.",
            "ì¸ê³µì§€ëŠ¥ì˜ ë¯¸ë˜ì— ëŒ€í•œ ë‹¹ì‹ ì˜ ìƒê°ì€?",
            "íš¨ê³¼ì ì¸ í•™ìŠµ ë°©ë²•ì„ ì•Œë ¤ì£¼ì„¸ìš”.",
            "í™˜ê²½ ë³´í˜¸ë¥¼ ìœ„í•´ ê°œì¸ì´ í•  ìˆ˜ ìˆëŠ” ì¼ë“¤ì€?"
        ]
        
        print("\nğŸ¯ ì‚¬ìš© ë°©ë²•ì„ ì„ íƒí•˜ì„¸ìš”:")
        print("1. ì§ì ‘ ì§ˆë¬¸ ì…ë ¥")
        print("2. ìƒ˜í”Œ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©")
        print("3. ëŒ€í™”í˜• ì±„íŒ…")
        
        choice = input("\nì„ íƒ (1-3): ").strip()
        
        if choice == "1":
            # ì§ì ‘ ì…ë ¥
            user_prompt = input("\nì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”: ")
            print("\nğŸ¤– ìƒì„± ì¤‘...")
            
            response = chat_bot.generate_text(user_prompt)
            print(f"\nğŸŒ¸ Bllossom:\n{response}")
            
        elif choice == "2":
            # ìƒ˜í”Œ í”„ë¡¬í”„íŠ¸
            print("\nğŸ“ ìƒ˜í”Œ í”„ë¡¬í”„íŠ¸ë“¤:")
            for i, prompt in enumerate(sample_prompts, 1):
                print(f"{i}. {prompt}")
            
            try:
                idx = int(input("\në²ˆí˜¸ë¥¼ ì„ íƒí•˜ì„¸ìš” (1-5): ")) - 1
                if 0 <= idx < len(sample_prompts):
                    selected_prompt = sample_prompts[idx]
                    print(f"\nì§ˆë¬¸: {selected_prompt}")
                    print("ğŸ¤– ìƒì„± ì¤‘...")
                    
                    response = chat_bot.generate_text(selected_prompt)
                    print(f"\nğŸŒ¸ Bllossom:\n{response}")
                else:
                    print("âŒ ì˜ëª»ëœ ë²ˆí˜¸ì…ë‹ˆë‹¤.")
            except ValueError:
                print("âŒ ìˆ«ìë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
                
        elif choice == "3":
            # ëŒ€í™”í˜• ì±„íŒ…
            print("\nğŸ’¬ ëŒ€í™”ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤. 'quit' ë˜ëŠ” 'ì¢…ë£Œ'ë¥¼ ì…ë ¥í•˜ë©´ ëë‚©ë‹ˆë‹¤.")
            print("-" * 50)
            
            conversation_count = 0
            while True:
                user_input = input(f"\nğŸ‘¤ You: ").strip()
                
                if user_input.lower() in ['quit', 'exit', 'ì¢…ë£Œ', 'q']:
                    print("ğŸ‘‹ ëŒ€í™”ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                    break
                
                if not user_input:
                    print("ğŸ’¡ ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
                    continue
                
                print("ğŸ¤– ìƒê° ì¤‘...", end="", flush=True)
                
                try:
                    # ì˜¨ë„ë¥¼ ì¡°ê¸ˆì”© ë³€í™”ì‹œì¼œì„œ ë‹¤ì–‘í•œ ë‹µë³€ ìƒì„±
                    temp = 0.7 + (conversation_count % 3) * 0.1
                    response = chat_bot.generate_text(user_input, temperature=temp)
                    print(f"\rğŸŒ¸ Bllossom: {response}")
                    
                    conversation_count += 1
                    
                except Exception as e:
                    print(f"\râŒ ìƒì„± ì˜¤ë¥˜: {e}")
        
        else:
            print("âŒ ì˜ëª»ëœ ì„ íƒì…ë‹ˆë‹¤.")
    
    except KeyboardInterrupt:
        print("\n\nâ¹ï¸ í”„ë¡œê·¸ë¨ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")

def quick_test():
    """ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ í•¨ìˆ˜"""
    print("ğŸ§ª ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘...")
    
    try:
        chat_bot = SimpleBlossomChat()
        
        test_prompts = [
            "ì•ˆë…•í•˜ì„¸ìš”!",
            "íŒŒì´ì¬ì´ë€ ë¬´ì—‡ì¸ê°€ìš”?",
            "ì¢‹ì€ í•˜ë£¨ ë³´ë‚´ì„¸ìš”!"
        ]
        
        for i, prompt in enumerate(test_prompts, 1):
            print(f"\nğŸ” í…ŒìŠ¤íŠ¸ {i}: {prompt}")
            response = chat_bot.generate_text(prompt, max_length=100)
            print(f"ğŸŒ¸ ì‘ë‹µ: {response}")
            
    except Exception as e:
        print(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")

def batch_generation():
    """ì—¬ëŸ¬ ì§ˆë¬¸ ë°°ì¹˜ ì²˜ë¦¬"""
    print("ğŸ“¦ ë°°ì¹˜ ìƒì„± ëª¨ë“œ")
    
    try:
        chat_bot = SimpleBlossomChat()
        
        questions = []
        print("ì§ˆë¬¸ë“¤ì„ ì…ë ¥í•˜ì„¸ìš” (ë¹ˆ ì¤„ë¡œ ì¢…ë£Œ):")
        
        while True:
            question = input(f"ì§ˆë¬¸ {len(questions)+1}: ").strip()
            if not question:
                break
            questions.append(question)
        
        if not questions:
            print("âŒ ì§ˆë¬¸ì´ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        print(f"\nğŸ”„ {len(questions)}ê°œ ì§ˆë¬¸ ì²˜ë¦¬ ì¤‘...")
        
        for i, question in enumerate(questions, 1):
            print(f"\nğŸ“ ì§ˆë¬¸ {i}: {question}")
            response = chat_bot.generate_text(question)
            print(f"ğŸŒ¸ ë‹µë³€: {response}")
            print("-" * 30)
            
    except Exception as e:
        print(f"âŒ ë°°ì¹˜ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")

if __name__ == "__main__":
    # ì‹¤í–‰ ëª¨ë“œ ì„ íƒ
    import sys
    
    if len(sys.argv) > 1:
        mode = sys.argv[1]
        if mode == "test":
            quick_test()
        elif mode == "batch":
            batch_generation()
        else:
            main()
    else:
        main()