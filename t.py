#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
í•œêµ­ì–´ Bllossom ëª¨ë¸ í…ìŠ¤íŠ¸ ìƒì„± ì˜ˆì œ
ê°„ë‹¨í•˜ê³  ì‹¤ìš©ì ì¸ í…ìŠ¤íŠ¸ ìƒì„± ë°ëª¨
"""

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
import warnings
warnings.filterwarnings('ignore')

class SimpleBlossomChat:
    def __init__(self, model_name=None):
        """ê°„ë‹¨í•œ í•œêµ­ì–´ í…ìŠ¤íŠ¸ ìƒì„±ê¸° ì´ˆê¸°í™”"""
        # ì—¬ëŸ¬ ëª¨ë¸ ì˜µì…˜ ì œê³µ
        self.available_models = {
            "bllossom": "Bllossom/llama-3.1-Korean-Bllossom-Vision-8B",
            "bllossom_text": "Bllossom/llama-3.2-Korean-Bllossom-3B",  # í…ìŠ¤íŠ¸ ì „ìš©
            "eeve": "yanolja/EEVE-Korean-Instruct-10.8B-v1.0",  # ëŒ€ì•ˆ í•œêµ­ì–´ ëª¨ë¸
            "solar": "upstage/SOLAR-10.7B-Instruct-v1.0"  # ë” ê°€ë²¼ìš´ ëŒ€ì•ˆ
        }
        
        # ëª¨ë¸ ì„ íƒ
        if model_name is None:
            model_name = self.select_model()
        
        self.model_name = self.available_models.get(model_name, model_name)
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        print(f"ğŸŒ¸ ëª¨ë¸ ë¡œë”© ì¤‘: {self.model_name}")
        print(f"ğŸ“± ë””ë°”ì´ìŠ¤: {self.device}")
        
        self.load_model()
    
    def select_model(self):
        """ì‚¬ìš©í•  ëª¨ë¸ ì„ íƒ"""
        print("\nğŸ¤– ì‚¬ìš©í•  ëª¨ë¸ì„ ì„ íƒí•˜ì„¸ìš”:")
        print("1. bllossom - Bllossom Vision (ë¹„ì „ ê¸°ëŠ¥ í¬í•¨, í° ëª¨ë¸)")
        print("2. bllossom_text - Bllossom í…ìŠ¤íŠ¸ ì „ìš© (3B, ê°€ë²¼ì›€)")  
        print("3. eeve - EEVE í•œêµ­ì–´ ëª¨ë¸ (ì•ˆì •ì )")
        print("4. solar - SOLAR ëª¨ë¸ (ë¹ ë¦„)")
        
        choice = input("ì„ íƒ (1-4, ê¸°ë³¸ê°’: 2): ").strip()
        
        model_map = {"1": "bllossom", "2": "bllossom_text", "3": "eeve", "4": "solar"}
        return model_map.get(choice, "bllossom_text")
    
    def load_model(self):
        """ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ (ì—ëŸ¬ ë³µêµ¬ ê¸°ëŠ¥ í¬í•¨)"""
        max_retries = 3
        
        for attempt in range(max_retries):
            try:
                print(f"ğŸ“¥ ì‹œë„ {attempt + 1}/{max_retries}: í† í¬ë‚˜ì´ì € ë¡œë”©...")
                
                # í† í¬ë‚˜ì´ì € ë¡œë“œ
                self.tokenizer = AutoTokenizer.from_pretrained(
                    self.model_name,
                    trust_remote_code=True,
                    use_fast=False  # í˜¸í™˜ì„±ì„ ìœ„í•´
                )
                
                # íŒ¨ë”© í† í° ì„¤ì •
                if self.tokenizer.pad_token is None:
                    self.tokenizer.pad_token = self.tokenizer.eos_token
                
                print(f"ğŸ“¥ ëª¨ë¸ ë¡œë”© ì¤‘...")
                
                # ê¸°ë³¸ ëª¨ë¸ ë¡œë“œ ì„¤ì • (ì•ˆì „í•œ ì„¤ì •)
                model_kwargs = {
                    "trust_remote_code": True,
                    "torch_dtype": torch.float16 if torch.cuda.is_available() else torch.float32,
                    "low_cpu_mem_usage": True
                }
                
                # ë‹¨ê³„ë³„ ë¡œë”© ì‹œë„
                if torch.cuda.is_available():
                    print("ğŸš€ GPU ê°ì§€ë¨. GPU ìµœì í™” ì‹œë„...")
                    try:
                        # ë¨¼ì € ê°„ë‹¨í•œ GPU ë¡œë”© ì‹œë„
                        model_kwargs["device_map"] = "auto"
                        
                        # ë©”ëª¨ë¦¬ê°€ ì¶©ë¶„í•˜ì§€ ì•Šìœ¼ë©´ ì–‘ìí™” ì‹œë„
                        if attempt > 0:  # ì²« ë²ˆì§¸ ì‹œë„ ì‹¤íŒ¨ì‹œ
                            try:
                                from transformers import BitsAndBytesConfig
                                quantization_config = BitsAndBytesConfig(
                                    load_in_8bit=True  # 4bit ëŒ€ì‹  8bit ì‚¬ìš© (ë” ì•ˆì •ì )
                                )
                                model_kwargs["quantization_config"] = quantization_config
                                print("ğŸ”§ 8bit ì–‘ìí™” ì ìš©")
                            except ImportError:
                                print("âš ï¸ bitsandbytes ì—†ìŒ. CPUë¡œ ë¡œë”© ì‹œë„")
                                model_kwargs = {
                                    "trust_remote_code": True,
                                    "torch_dtype": torch.float32
                                }
                    except Exception as gpu_error:
                        print(f"âš ï¸ GPU ë¡œë”© ì‹¤íŒ¨: {gpu_error}")
                        print("ğŸ”„ CPU ë¡œë”©ìœ¼ë¡œ ì „í™˜...")
                        model_kwargs = {
                            "trust_remote_code": True,
                            "torch_dtype": torch.float32
                        }
                
                # ëª¨ë¸ ë¡œë“œ
                self.model = AutoModelForCausalLM.from_pretrained(
                    self.model_name,
                    **model_kwargs
                )
                
                # CPUë¡œ ì´ë™ (í•„ìš”ì‹œ)
                if not torch.cuda.is_available():
                    self.model = self.model.to("cpu")
                
                print("âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ!")
                self.print_model_info()
                return
                
            except Exception as e:
                print(f"âŒ ë¡œë”© ì‹¤íŒ¨ (ì‹œë„ {attempt + 1}): {e}")
                
                if attempt < max_retries - 1:
                    print("ğŸ”„ ë‹¤ë¥¸ ì„¤ì •ìœ¼ë¡œ ì¬ì‹œë„...")
                    # ë‹¤ìŒ ì‹œë„ë¥¼ ìœ„í•´ ë” ì•ˆì „í•œ ì„¤ì • ì‚¬ìš©
                    if "bllossom" in self.model_name.lower():
                        # Bllossom ëª¨ë¸ì´ ì‹¤íŒ¨í•˜ë©´ ë” ê°„ë‹¨í•œ ëª¨ë¸ë¡œ ì „í™˜
                        self.model_name = self.available_models["solar"]
                        print(f"ğŸ”„ ëŒ€ì•ˆ ëª¨ë¸ë¡œ ì „í™˜: {self.model_name}")
                else:
                    print("âŒ ëª¨ë“  ì‹œë„ ì‹¤íŒ¨")
                    print("ğŸ’¡ í•´ê²°ì±…:")
                    print("  1. ì¸í„°ë„· ì—°ê²° í™•ì¸")
                    print("  2. pip install transformers --upgrade")
                    print("  3. ë” ì‘ì€ ëª¨ë¸ ì‚¬ìš©")
                    raise
    
    def print_model_info(self):
        """ëª¨ë¸ ì •ë³´ ì¶œë ¥"""
        try:
            param_count = sum(p.numel() for p in self.model.parameters())
            print(f"ğŸ“Š íŒŒë¼ë¯¸í„° ìˆ˜: {param_count:,}")
            print(f"ğŸ’¾ ë””ë°”ì´ìŠ¤: {next(self.model.parameters()).device}")
            print(f"ğŸ“ ëª¨ë¸: {self.model_name.split('/')[-1]}")
        except:
            print("ğŸ“Š ëª¨ë¸ ì •ë³´ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    def generate_text(self, prompt, max_length=300, temperature=0.7):
        """í…ìŠ¤íŠ¸ ìƒì„± í•¨ìˆ˜ (ëª¨ë¸ë³„ ìµœì í™”)"""
        try:
            # ëª¨ë¸ì— ë”°ë¥¸ í”„ë¡¬í”„íŠ¸ í¬ë§·íŒ…
            if "bllossom" in self.model_name.lower():
                # Bllossom ëª¨ë¸ìš© í¬ë§·
                formatted_prompt = f"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n\n{prompt}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n"
            elif "eeve" in self.model_name.lower():
                # EEVE ëª¨ë¸ìš© í¬ë§·
                formatted_prompt = f"### ì§ˆë¬¸: {prompt}\n\n### ë‹µë³€:"
            elif "solar" in self.model_name.lower():
                # SOLAR ëª¨ë¸ìš© í¬ë§·
                formatted_prompt = f"### User:\n{prompt}\n\n### Assistant:\n"
            else:
                # ê¸°ë³¸ í¬ë§·
                formatted_prompt = f"ì§ˆë¬¸: {prompt}\në‹µë³€:"
            
            # í† í¬ë‚˜ì´ì§• (ì—ëŸ¬ ì²˜ë¦¬ í¬í•¨)
            try:
                inputs = self.tokenizer(
                    formatted_prompt, 
                    return_tensors="pt",
                    truncation=True,
                    max_length=512,
                    padding=False
                )
            except Exception as tokenize_error:
                print(f"âš ï¸ í† í¬ë‚˜ì´ì§• ì˜¤ë¥˜: {tokenize_error}")
                # ê°„ë‹¨í•œ í¬ë§·ìœ¼ë¡œ ì¬ì‹œë„
                inputs = self.tokenizer(prompt, return_tensors="pt", truncation=True, max_length=512)
            
            # ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
            device = next(self.model.parameters()).device
            inputs = {k: v.to(device) for k, v in inputs.items()}
            
            # ìƒì„± íŒŒë¼ë¯¸í„° ì„¤ì • (ì•ˆì „í•œ ê°’ë“¤)
            generation_config = {
                "max_length": len(inputs['input_ids'][0]) + max_length,
                "min_length": len(inputs['input_ids'][0]) + 10,
                "temperature": temperature,
                "do_sample": True,
                "top_p": 0.9,
                "top_k": 50,
                "repetition_penalty": 1.1,
                "pad_token_id": self.tokenizer.eos_token_id,
                "eos_token_id": self.tokenizer.eos_token_id,
                "no_repeat_ngram_size": 2,
                "early_stopping": True
            }
            
            # í…ìŠ¤íŠ¸ ìƒì„±
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    **generation_config
                )
            
            # ë””ì½”ë”©
            response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            
            # í”„ë¡¬í”„íŠ¸ ë¶€ë¶„ ì œê±° ë° ì •ë¦¬
            if "assistant<|end_header_id|>" in response:
                response = response.split("assistant<|end_header_id|>")[-1].strip()
            elif "### ë‹µë³€:" in response:
                response = response.split("### ë‹µë³€:")[-1].strip()
            elif "### Assistant:" in response:
                response = response.split("### Assistant:")[-1].strip()
            elif "ë‹µë³€:" in response:
                response = response.split("ë‹µë³€:")[-1].strip()
            elif formatted_prompt in response:
                response = response.replace(formatted_prompt, "").strip()
            
            # ë¹ˆ ì‘ë‹µ ë°©ì§€
            if not response.strip():
                response = "ì£„ì†¡í•©ë‹ˆë‹¤. ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ì§ˆë¬¸ì„ ì‹œë„í•´ë³´ì„¸ìš”."
            
            return response
            
        except Exception as e:
            error_msg = f"í…ìŠ¤íŠ¸ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"
            print(f"âŒ {error_msg}")
            return error_msg

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸŒ¸ í•œêµ­ì–´ Bllossom í…ìŠ¤íŠ¸ ìƒì„± ë°ëª¨")
    print("=" * 40)
    
    try:
        # ëª¨ë¸ ì´ˆê¸°í™”
        chat_bot = SimpleBlossomChat()
        
        # ìƒ˜í”Œ í”„ë¡¬í”„íŠ¸ë“¤
        sample_prompts = [
            "íŒŒì´ì¬ í”„ë¡œê·¸ë˜ë°ì˜ ì¥ì ì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
            "í•œêµ­ì˜ ì „í†µ ìŒì‹ì— ëŒ€í•´ ì†Œê°œí•´ì£¼ì„¸ìš”.",
            "ì¸ê³µì§€ëŠ¥ì˜ ë¯¸ë˜ì— ëŒ€í•œ ë‹¹ì‹ ì˜ ìƒê°ì€?",
            "íš¨ê³¼ì ì¸ í•™ìŠµ ë°©ë²•ì„ ì•Œë ¤ì£¼ì„¸ìš”.",
            "í™˜ê²½ ë³´í˜¸ë¥¼ ìœ„í•´ ê°œì¸ì´ í•  ìˆ˜ ìˆëŠ” ì¼ë“¤ì€?"
        ]
        
        print("\nğŸ¯ ì‚¬ìš© ë°©ë²•ì„ ì„ íƒí•˜ì„¸ìš”:")
        print("1. ì§ì ‘ ì§ˆë¬¸ ì…ë ¥")
        print("2. ìƒ˜í”Œ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©")
        print("3. ëŒ€í™”í˜• ì±„íŒ…")
        
        choice = input("\nì„ íƒ (1-3): ").strip()
        
        if choice == "1":
            # ì§ì ‘ ì…ë ¥
            user_prompt = input("\nì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”: ")
            print("\nğŸ¤– ìƒì„± ì¤‘...")
            
            response = chat_bot.generate_text(user_prompt)
            print(f"\nğŸŒ¸ Bllossom:\n{response}")
            
        elif choice == "2":
            # ìƒ˜í”Œ í”„ë¡¬í”„íŠ¸
            print("\nğŸ“ ìƒ˜í”Œ í”„ë¡¬í”„íŠ¸ë“¤:")
            for i, prompt in enumerate(sample_prompts, 1):
                print(f"{i}. {prompt}")
            
            try:
                idx = int(input("\në²ˆí˜¸ë¥¼ ì„ íƒí•˜ì„¸ìš” (1-5): ")) - 1
                if 0 <= idx < len(sample_prompts):
                    selected_prompt = sample_prompts[idx]
                    print(f"\nì§ˆë¬¸: {selected_prompt}")
                    print("ğŸ¤– ìƒì„± ì¤‘...")
                    
                    response = chat_bot.generate_text(selected_prompt)
                    print(f"\nğŸŒ¸ Bllossom:\n{response}")
                else:
                    print("âŒ ì˜ëª»ëœ ë²ˆí˜¸ì…ë‹ˆë‹¤.")
            except ValueError:
                print("âŒ ìˆ«ìë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
                
        elif choice == "3":
            # ëŒ€í™”í˜• ì±„íŒ…
            print("\nğŸ’¬ ëŒ€í™”ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤. 'quit' ë˜ëŠ” 'ì¢…ë£Œ'ë¥¼ ì…ë ¥í•˜ë©´ ëë‚©ë‹ˆë‹¤.")
            print("-" * 50)
            
            conversation_count = 0
            while True:
                user_input = input(f"\nğŸ‘¤ You: ").strip()
                
                if user_input.lower() in ['quit', 'exit', 'ì¢…ë£Œ', 'q']:
                    print("ğŸ‘‹ ëŒ€í™”ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                    break
                
                if not user_input:
                    print("ğŸ’¡ ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
                    continue
                
                print("ğŸ¤– ìƒê° ì¤‘...", end="", flush=True)
                
                try:
                    # ì˜¨ë„ë¥¼ ì¡°ê¸ˆì”© ë³€í™”ì‹œì¼œì„œ ë‹¤ì–‘í•œ ë‹µë³€ ìƒì„±
                    temp = 0.7 + (conversation_count % 3) * 0.1
                    response = chat_bot.generate_text(user_input, temperature=temp)
                    print(f"\rğŸŒ¸ Bllossom: {response}")
                    
                    conversation_count += 1
                    
                except Exception as e:
                    print(f"\râŒ ìƒì„± ì˜¤ë¥˜: {e}")
        
        else:
            print("âŒ ì˜ëª»ëœ ì„ íƒì…ë‹ˆë‹¤.")
    
    except KeyboardInterrupt:
        print("\n\nâ¹ï¸ í”„ë¡œê·¸ë¨ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")

def quick_test():
    """ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ í•¨ìˆ˜ (ì˜¤ë¥˜ ë³µêµ¬ í¬í•¨)"""
    print("ğŸ§ª ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘...")
    
    try:
        # ê°€ì¥ ê°€ë²¼ìš´ ëª¨ë¸ë¡œ í…ŒìŠ¤íŠ¸
        chat_bot = SimpleBlossomChat("solar")  # SOLAR ëª¨ë¸ ì‚¬ìš©
        
        test_prompts = [
            "ì•ˆë…•í•˜ì„¸ìš”!",
            "íŒŒì´ì¬ì´ë€ ë¬´ì—‡ì¸ê°€ìš”?",
            "ì¢‹ì€ í•˜ë£¨ ë³´ë‚´ì„¸ìš”!"
        ]
        
        for i, prompt in enumerate(test_prompts, 1):
            print(f"\nğŸ” í…ŒìŠ¤íŠ¸ {i}: {prompt}")
            try:
                response = chat_bot.generate_text(prompt, max_length=100)
                print(f"ğŸŒ¸ ì‘ë‹µ: {response}")
            except Exception as e:
                print(f"âŒ í…ŒìŠ¤íŠ¸ {i} ì‹¤íŒ¨: {e}")
                
    except Exception as e:
        print(f"âŒ ì „ì²´ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        print("ğŸ’¡ í•´ê²°ì±…: pip install transformers torch --upgrade")

def batch_generation():
    """ì—¬ëŸ¬ ì§ˆë¬¸ ë°°ì¹˜ ì²˜ë¦¬"""
    print("ğŸ“¦ ë°°ì¹˜ ìƒì„± ëª¨ë“œ")
    
    try:
        chat_bot = SimpleBlossomChat()
        
        questions = []
        print("ì§ˆë¬¸ë“¤ì„ ì…ë ¥í•˜ì„¸ìš” (ë¹ˆ ì¤„ë¡œ ì¢…ë£Œ):")
        
        while True:
            question = input(f"ì§ˆë¬¸ {len(questions)+1}: ").strip()
            if not question:
                break
            questions.append(question)
        
        if not questions:
            print("âŒ ì§ˆë¬¸ì´ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        print(f"\nğŸ”„ {len(questions)}ê°œ ì§ˆë¬¸ ì²˜ë¦¬ ì¤‘...")
        
        for i, question in enumerate(questions, 1):
            print(f"\nğŸ“ ì§ˆë¬¸ {i}: {question}")
            response = chat_bot.generate_text(question)
            print(f"ğŸŒ¸ ë‹µë³€: {response}")
            print("-" * 30)
            
    except Exception as e:
        print(f"âŒ ë°°ì¹˜ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")

if __name__ == "__main__":
    # ì‹¤í–‰ ëª¨ë“œ ì„ íƒ
    import sys
    
    print("ğŸŒ¸ í•œêµ­ì–´ AI í…ìŠ¤íŠ¸ ìƒì„±ê¸°")
    print("=" * 30)
    
    if len(sys.argv) > 1:
        mode = sys.argv[1]
        if mode == "test":
            quick_test()
        elif mode == "batch":
            batch_generation()
        else:
            main()
    else:
        # ì•ˆì „í•œ ì‹œì‘ì„ ìœ„í•œ ì‹œìŠ¤í…œ ì²´í¬
        print("ğŸ” ì‹œìŠ¤í…œ ì²´í¬ ì¤‘...")
        
        try:
            # PyTorch ì²´í¬
            print(f"âœ… PyTorch: {torch.__version__}")
            print(f"âœ… CUDA ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.is_available()}")
            
            # Transformers ì²´í¬
            import transformers
            print(f"âœ… Transformers: {transformers.__version__}")
            
            print("\nğŸš€ ëª¨ë“  ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤!")
            main()
            
        except ImportError as e:
            print(f"âŒ ë¼ì´ë¸ŒëŸ¬ë¦¬ ëˆ„ë½: {e}")
            print("\nğŸ’¡ ë‹¤ìŒ ëª…ë ¹ì–´ë¡œ ì„¤ì¹˜í•˜ì„¸ìš”:")
            print("pip install torch transformers accelerate")
        except Exception as e:
            print(f"âŒ ì‹œìŠ¤í…œ ì²´í¬ ì‹¤íŒ¨: {e}")
            print("ğŸ”„ ê¸°ë³¸ ëª¨ë“œë¡œ ì‹¤í–‰ ì‹œë„...")
            main()